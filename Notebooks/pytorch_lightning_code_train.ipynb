{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os, copy, glob\n",
    "import sys\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "from monai.losses import *\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r\"E:\\Projects\\Deep_Learning\\Car_Segmentation\\Utils\")\n",
    "\n",
    "from segmentation_utils import SegmentationDataset, DataAugmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../Data/car_dataset\"\n",
    "BASE_PROJECT_DIR = \"E:/Projects/Deep_Learning/Car_Segmentation/\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 150\n",
    "ENCODER =  \"xception\"\n",
    "MODEL_TYPE = \"Unet\"\n",
    "EXPERIMENT_NAME = \"Car_Segmentation\"\n",
    "LOSS_NAME = \"DiceCELoss\"\n",
    "LOSS_FN =  DiceCELoss(include_background = True, softmax= True, lambda_dice=0.1, lambda_ce=0.9)\n",
    "LEARNING_RATE =  0.0001\n",
    "MODEL_VERSION = 1\n",
    "RUN_NAME = EXPERIMENT_NAME + \"_\" + MODEL_TYPE + \"_\" + ENCODER + \"_\" + LOSS_NAME + \"_\" + str(MODEL_VERSION)\n",
    "print(RUN_NAME)\n",
    "# OPTIMIZER = {\"name\": \"AdamW\"},\n",
    "# SCHEDULER = {\"name\": \"ReduceLROnPlateau\",\n",
    "#                 \"mode\": \"min\",\n",
    "#                 \"patience\": 5,\n",
    "#                 \"cooldown\": 1,\n",
    "#                 \"verbose\": True}\n",
    "CALLBACKS =  [{\"name\": \"EarlyStopping\",\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"min_delta\": 0.001,\n",
    "                \"patience\": 15,\n",
    "                \"verbose\": True,\n",
    "                \"mode\": \"min\"}]\n",
    "IN_CHANNELS = 3\n",
    "NUM_CLASSES = 5\n",
    "PRECISION = 32\n",
    "TRAIN_BOOL = True\n",
    "MODEL_WEIGHTS_PATH = \"\"\n",
    "OVERSAMPLE = False\n",
    "NUM_CORES = 0\n",
    "\n",
    "AUGMENTATIONS = [\n",
    "            {\n",
    "                \"name\":\"RandomAffine\",\n",
    "                \"degrees\":360,\n",
    "                \"align_corners\":True,\n",
    "                \"p\":0.6\n",
    "            },\n",
    "            {\n",
    "                \"name\":\"RandomHorizontalFlip\",\n",
    "                \"p\":0.6\n",
    "            },\n",
    "            {\n",
    "                \"name\":\"RandomVerticalFlip\",\n",
    "                \"p\":0.6\n",
    "            },\n",
    "            {\n",
    "                \"name\":\"RandomRotation\",\n",
    "                \"degrees\":360,\n",
    "                \"p\":0.6\n",
    "            }\n",
    "            ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics.functional.classification\n",
    "import torchmetrics.functional.segmentation\n",
    "\n",
    "\n",
    "class SegmentationModule(pl.LightningModule):\n",
    "    def __init__(self, augmentations, in_channels = 1, out_channels = 1, encoder_name = 'resnet34', encoder_weights = 'imagenet', model_type = 'Unet',\n",
    "                    img_size = 512, loss_function = {\"name\":'CELoss', \"ALPHA\":1.0,\"BETA\":1.0}, lr = 0.01):\n",
    "        super().__init__()\n",
    "        # self.scheduler = scheduler\n",
    "        # self.optimizer = optimizer\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels # number of Classes \n",
    "        self.encoder_name = encoder_name\n",
    "        self.encoder_weights = encoder_weights\n",
    "        self.model_type = model_type\n",
    "        self.img_size = img_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.augmentations = DataAugmentation(augmentations)\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "        exec(f\"self.model = smp.{self.model_type}(encoder_name=self.encoder_name, encoder_weights=self.encoder_weights, \"\n",
    "                    f\"in_channels=self.in_channels, classes=self.out_channels)\")\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        img, mask = batch\n",
    "        \n",
    "        ## Apply augmentation to training data\n",
    "        img, mask = self.augmentations(img, mask)\n",
    "        \n",
    "        pred_mask = self(img)\n",
    "        \n",
    "        loss = self.loss_function(pred_mask, mask)\n",
    "        \n",
    "        pred_mask = torch.softmax(pred_mask,dim=1).squeeze()\n",
    "        mask = mask.squeeze()\n",
    "        # mask = torch.argmax(mask, dim=1)\n",
    "        \n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask, mask.int(), mode='multilabel', threshold=0.5, num_classes=self.out_channels)\n",
    "        iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        \n",
    "        dice_score = torchmetrics.functional.classification.multiclass_f1_score(preds = pred_mask, target = mask.int(), num_classes = self.out_channels)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log('train_iou', iou_score, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log('train_dice', dice_score, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        img, mask = batch\n",
    "        \n",
    "        pred_mask = self(img)\n",
    "        \n",
    "        loss = self.loss_function(pred_mask, mask)\n",
    "        \n",
    "        pred_mask = torch.softmax(pred_mask,dim=1).squeeze()\n",
    "        mask = mask.squeeze()\n",
    "        # mask = torch.argmax(mask, dim=1)\n",
    "        \n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask, mask.int(), mode='multilabel', threshold=0.5, num_classes=self.out_channels)\n",
    "        iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        \n",
    "        dice_score = torchmetrics.functional.classification.multiclass_f1_score(preds = pred_mask, target = mask.int(), num_classes = self.out_channels)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log('val_iou', iou_score, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log('val_dice', dice_score, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        scheduler = {'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, mode = \"min\", patience =  3, cooldown = 1, verbose=True),\n",
    "                        'monitor': 'val_loss'}\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_logger = MLFlowLogger(\n",
    "    experiment_name= EXPERIMENT_NAME,\n",
    "    run_name= RUN_NAME,\n",
    "    save_dir='E:/Projects/Deep_Learning/Car_Segmentation/mlruns',\n",
    "    log_model = True,\n",
    "    artifact_location = 'E:/Projects/Deep_Learning/Car_Segmentation/artifacts'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"---Training Staring---\")\n",
    "    \n",
    "    # Define datasets\n",
    "    whole_dataset = SegmentationDataset(dirPath=DATASET_PATH, imageDir='images/', masksDir='masks/', img_size=IMG_SIZE)\n",
    "    \n",
    "    train_size = int(0.8 * len(whole_dataset))\n",
    "    val_size = len(whole_dataset) - train_size\n",
    "    train_ds, val_ds = torch.utils.data.random_split(whole_dataset, [train_size, val_size])\n",
    "    \n",
    "    print(f\"Length of Train dataset = {len(train_ds)}\")\n",
    "    print(f\"Length of Val dataset = {len(val_ds)}\")\n",
    "\n",
    "    # Define DataLoaders\n",
    "    train_dataloader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=False)\n",
    "    val_dataloader = DataLoader(val_ds, BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "    print(\"Created Train and Val dataloaders\")\n",
    "    \n",
    "    model = SegmentationModule(augmentations=copy.deepcopy(AUGMENTATIONS), in_channels=IN_CHANNELS, out_channels=NUM_CLASSES, encoder_name=ENCODER, encoder_weights=None, model_type=MODEL_TYPE, img_size=IMG_SIZE, \n",
    "                                loss_function=LOSS_FN, lr=LEARNING_RATE)\n",
    "    \n",
    "    # Create model directory\n",
    "    os.makedirs(f'{BASE_PROJECT_DIR}/Model Checkpoints/{RUN_NAME}/', exist_ok=True)\n",
    "    print(f\"Created directory: {BASE_PROJECT_DIR}/Model Checkpoints/{RUN_NAME}/\")\n",
    "    \n",
    "    callback_list = []\n",
    "    early_stopping = pl.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0.001, patience = 15, verbose = True, mode = \"min\")\n",
    "    model_checkpoint = pl.callbacks.ModelCheckpoint(monitor = 'val_loss', dirpath = f'{BASE_PROJECT_DIR}/Model Checkpoints/{RUN_NAME}/', verbose = True)\n",
    "    \n",
    "    callback_list.append(early_stopping)\n",
    "    callback_list.append(model_checkpoint)\n",
    "    \n",
    "    trainer = pl.Trainer(accelerator='gpu', devices='auto', accumulate_grad_batches= max(1, 16 // BATCH_SIZE),\n",
    "                            benchmark=True, precision=PRECISION, min_epochs=5, max_epochs=EPOCHS,\n",
    "                            logger=mlflow_logger, num_sanity_val_steps=5, callbacks=callback_list)\n",
    "    \n",
    "    # Train model\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    print(\"\\nTraining finished\")\n",
    "    \n",
    "    # -- Save model --\n",
    "    # Find latest checkpoint\n",
    "    best_chkpt = max(glob.glob(f'{BASE_PROJECT_DIR}/Model Checkpoints/{RUN_NAME}/*.ckpt'),\n",
    "                    key=os.path.getctime)\n",
    "    print(f\"Found latest checkpoint at:\\n{best_chkpt}\")\n",
    "\n",
    "    # Load latest checkpoint\n",
    "    model = model.load_from_checkpoint(best_chkpt)\n",
    "\n",
    "    # Convert the model and weights to torchscript and Save\n",
    "    model.eval()\n",
    "\n",
    "    torch.save(model.state_dict(), f'{BASE_PROJECT_DIR}/Model Checkpoints/{RUN_NAME}/{RUN_NAME}_weights.pth')\n",
    "    print(f\"Model Weights saved in {BASE_PROJECT_DIR}/Model Checkpoints/{RUN_NAME}/\")\n",
    "\n",
    "    try:\n",
    "        model.to_torchscript(\n",
    "            file_path=f'{BASE_PROJECT_DIR}/Model Checkpoints/{RUN_NAME}/{RUN_NAME}_traceModel.pth',\n",
    "            method='trace', example_inputs=torch.rand(1, IN_CHANNELS, IMG_SIZE, IMG_SIZE))\n",
    "        print(f\"TorchScript Trace Model saved in {BASE_PROJECT_DIR}/Model Checkpoints/{RUN_NAME}/\")\n",
    "    except:\n",
    "        print(f\"Trace Model could not be created for {RUN_NAME} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

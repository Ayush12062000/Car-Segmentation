{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8005\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"Car Segmentation\")\n",
    "\n",
    "# Auto Log on MLFlow\n",
    "# mlflow.pytorch.autolog() # Not working in Vanilla Pytorch code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import kornia\n",
    "from kornia.augmentation import *\n",
    "import cv2\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Dict, Optional\n",
    "from monai.losses.dice import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "dataset_path = \"../Data/car_dataset\"\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 150\n",
    "NUM_WORKERS = multiprocessing.cpu_count()\n",
    "IMAGE_SIZE = 512\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = True\n",
    "\n",
    "LOSS_NAME = 'diceceloss'\n",
    "LOSS_FUNCTION = DiceCELoss(include_background = True, softmax= True, lambda_dice=0.1, lambda_ce=0.9)\n",
    "\n",
    "AUGMENTATIONS = [\n",
    "            {\n",
    "                \"name\":\"RandomAffine\",\n",
    "                \"degrees\":360,\n",
    "                \"align_corners\":True,\n",
    "                \"p\":0.6\n",
    "            },\n",
    "            {\n",
    "                \"name\":\"RandomHorizontalFlip\",\n",
    "                \"p\":0.6\n",
    "            },\n",
    "            {\n",
    "                \"name\":\"RandomVerticalFlip\",\n",
    "                \"p\":0.6\n",
    "            },\n",
    "            {\n",
    "                \"name\":\"RandomRotation\",\n",
    "                \"degrees\":360,\n",
    "                \"p\":0.6\n",
    "            }\n",
    "            # {\n",
    "            #     \"name\":\"CustomPadding\",\n",
    "            #     \"padding\":100,\n",
    "            #     \"p\":0.6\n",
    "            # }\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "class PreProcess(torch.nn.Module):\n",
    "    '''\n",
    "    Class to convert numpy array into torch tensor\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    @torch.no_grad()  #disable gradients for efficiency\n",
    "    def forward(self, x: np.array) -> torch.tensor:\n",
    "        temp: np.ndarray = np.asarray(x) # HxWxC\n",
    "        out: torch.tensor = kornia.image_to_tensor(temp, keepdim=True)  # CxHxW\n",
    "        \n",
    "        return out.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class SegmentationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dirPath= r'../data', imageDir='images', masksDir='masks', img_size=512):\n",
    "        self.imgDirPath = os.path.join(dirPath, imageDir)\n",
    "        self.maskDirPath = os.path.join(dirPath, masksDir)\n",
    "        self.img_size = img_size\n",
    "        self.nameImgFile = sorted(os.listdir(self.imgDirPath))\n",
    "        self.nameMaskFile = sorted(os.listdir(self.maskDirPath))\n",
    "        self.preprocess = PreProcess()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.nameImgFile)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        imgPath = os.path.join(self.imgDirPath, self.nameImgFile[index])\n",
    "        maskPath = os.path.join(self.maskDirPath, self.nameMaskFile[index])\n",
    "        \n",
    "        img = cv2.imread(imgPath, cv2.IMREAD_COLOR)\n",
    "        resized_img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "        \n",
    "        # Min-max scaling\n",
    "        imin, imax = resized_img.min(), resized_img.max()\n",
    "        resized_img = (resized_img-imin)/(imax-imin)\n",
    "        \n",
    "        img = self.preprocess(resized_img) \n",
    "        \n",
    "        mask = cv2.imread(maskPath, cv2.IMREAD_UNCHANGED)\n",
    "        resized_mask = cv2.resize(mask, (self.img_size, self.img_size))\n",
    "        \n",
    "        mask = self.preprocess(resized_mask)\n",
    "        \n",
    "        # Create a new tensor of shape (5, 256, 256) filled with zeros\n",
    "        output_mask = torch.zeros((5, self.img_size, self.img_size), dtype=torch.float)\n",
    "\n",
    "        # Populate the output mask tensor using one-hot encoding\n",
    "        \n",
    "        '''\n",
    "            0 - background\n",
    "            1 - car\n",
    "            2 - wheel\n",
    "            3 - light\n",
    "            4 - windows\n",
    "        '''\n",
    "        \n",
    "        for i in range(5):\n",
    "            output_mask[i] = (mask == i).float()\n",
    "        \n",
    "        return img, output_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPadding(AugmentationBase2D):\n",
    "    \"\"\"\n",
    "    Custom augmentation to add padding on all sides of an image.\n",
    "    \"\"\"\n",
    "    def __init__(self, padding: int, p: float = 1.0):\n",
    "        super(CustomPadding, self).__init__(p=p)\n",
    "        self.padding = padding\n",
    "        \n",
    "    def apply_transform(self, img: torch.Tensor, params: Dict[str, torch.Tensor], flags: Dict[str, Any], transform: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        # Calculate the size of the padded image\n",
    "        b, c, h, w = img.size()\n",
    "        padded_h, padded_w = h + 2*self.padding, w + 2*self.padding\n",
    "        \n",
    "        # Create a tensor filled with zeros as the new padded image\n",
    "        padded_img = torch.zeros(b, c, padded_h, padded_w)\n",
    "\n",
    "        # Insert the original image in the center of the padded image\n",
    "        padded_img[:, :, self.padding:h+self.padding, self.padding:w+self.padding] = img\n",
    "        \n",
    "        resize_padded_img = torch.nn.functional.interpolate(padded_img, size=(512, 512), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return resize_padded_img.to(\"cuda\")\n",
    "    \n",
    "    def apply_non_transform(self, img: torch.Tensor, params: Dict[str, torch.Tensor], flags: Dict[str, Any], transform: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        return img.to(\"cuda\")\n",
    "        \n",
    "    \n",
    "    def apply_transform_mask(self, mask: torch.Tensor, params: Dict[str, torch.Tensor], flags: Dict[str, Any], transform: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        # Calculate the size of the padded image\n",
    "        b, c, h, w = mask.size()\n",
    "        padded_h, padded_w = h + 2*self.padding, w + 2*self.padding\n",
    "        \n",
    "        # Create a tensor filled with zeros as the new padded image\n",
    "        padded_mask = torch.zeros(b, c, padded_h, padded_w)\n",
    "        \n",
    "        # Insert the original image in the center of the padded image\n",
    "        padded_mask[:, :, self.padding:h+self.padding, self.padding:w+self.padding] = mask\n",
    "        \n",
    "        resize_padded_mask = torch.nn.functional.interpolate(padded_mask, size=(512, 512), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return resize_padded_mask.to(\"cuda\")\n",
    "    \n",
    "    def apply_non_transform_mask(self, mask: torch.Tensor, params: Dict[str, torch.Tensor], flags: Dict[str, Any], transform: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        return mask.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation\n",
    "class DataAugmentation(torch.nn.Module):\n",
    "    '''\n",
    "    Augmentation from Kornai\n",
    "    - Works with Image and Mask tensor input.\n",
    "    - Returns \"Identity\" if no augmentations are passed.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, augmentations):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.augmentations = torch.nn.Identity()\n",
    "        \n",
    "        if len(augmentations) > 0:\n",
    "            self.augmentations = self._createAugmentationObject(augmentations)\n",
    "    \n",
    "    def _createAugmentationObject(self,augs):\n",
    "        aug_object_list = []\n",
    "        print(augs)\n",
    "        for aug in augs:\n",
    "            aug_name = aug['name']\n",
    "            aug.pop('name', None)\n",
    "            aug_object_list.append(\n",
    "                globals()[aug_name](**aug)\n",
    "                )\n",
    "            aug['name'] = aug_name\n",
    "        aug_container = kornia.augmentation.container.AugmentationSequential(*aug_object_list, data_keys=['input', 'mask'])\n",
    "        return aug_container\n",
    "    \n",
    "    @torch.no_grad()  # disable gradients for effiency\n",
    "    def forward(self, img, mask):\n",
    "        img, mask = self.augmentations(img, mask)\n",
    "        return img, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_mask_using_dataset_class(dataset, number_of_images = 10):\n",
    "    ag = DataAugmentation(augmentations = copy.deepcopy(AUGMENTATIONS))\n",
    "    \n",
    "    for idx in range(number_of_images):\n",
    "            img, mask = dataset[idx]\n",
    "            \n",
    "            img, mask = ag(img,mask)\n",
    "            \n",
    "            img = img.squeeze().cpu()\n",
    "            mask = mask.cpu()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fig, axes = plt.subplots(1, 3,figsize=(15,15)) \n",
    "                axes[0].imshow(img[0], cmap = 'gray')\n",
    "                axes[0].axis(\"off\")\n",
    "                axes[0].set_title(\"Original scan\", fontsize = 12)\n",
    "                axes[1].imshow(mask[0][0], cmap=\"copper\")\n",
    "                axes[1].axis(\"off\")\n",
    "                axes[1].set_title(\"Ground Truth\", fontsize = 12)\n",
    "                axes[2].imshow(img[0], cmap = 'gray')\n",
    "                axes[2].imshow(mask[0][0], alpha = 0.5, cmap = 'copper')\n",
    "                axes[2].axis(\"off\")\n",
    "                axes[2].set_title(\"Overlapped View\", fontsize = 12)\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "# plot_image_mask(val_ds, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot images and masks\n",
    "def plot_images_and_masks_dataloader(images, masks, num_images):\n",
    "    fig, axs = plt.subplots(num_images, 2, figsize=(10, 5 * num_images))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Plot the image\n",
    "        axs[i, 0].imshow(images[i].permute(1, 2, 0).cpu().numpy())  # Convert CHW to HWC format and to numpy\n",
    "        axs[i, 0].set_title(f'Image {i + 1}')\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        # Plot the mask\n",
    "        axs[i, 1].imshow(masks[i].squeeze().cpu().numpy(), cmap='gray')  # Convert C1H to H format and to numpy\n",
    "        axs[i, 1].set_title(f'Mask {i + 1}')\n",
    "        axs[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the DataLoader to get images and masks to plot, KEEP NUM_WORKER as 0\n",
    "'''\n",
    "for i, (img, mask) in enumerate(train_dataloader):\n",
    "    try:\n",
    "        print(f\"Batch {i + 1}:\")\n",
    "        print(\"Image batch shape:\", img.shape)\n",
    "        print(\"Mask batch shape:\", mask.shape)\n",
    "        \n",
    "        # for i in img:\n",
    "        #     print(i.shape)\n",
    "        #     i = i.permute(1,2,0)\n",
    "        #     print(\"lol = \",i.shape)\n",
    "        \n",
    "        # plot_images_and_masks(img, mask, min(2, img.size(0)))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i + 1}: {e}\")\n",
    "        continue\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),  # Bias is False as it will be cancelled out but BatchNorm\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),  # Bias is False as it will be cancelled out but BatchNorm\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels = 3, out_channels = 1, features = [64, 128, 256, 512]):\n",
    "        super(UNET, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        # Down part of Unet\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "        \n",
    "        # Up part of Unet\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                        nn.ConvTranspose2d(feature*2, feature, kernel_size = 2, stride = 2),\n",
    "                        )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "        \n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.finalconv = nn.Conv2d(features[0], out_channels, kernel_size = 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        \n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        \n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "            \n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size = skip_connection.shape[2:])\n",
    "            \n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "        \n",
    "        return self.finalconv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice Score Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(pred, gt_mask, threshold=0.5):\n",
    "    pred = (pred > threshold).float()\n",
    "    gt_mask = gt_mask.float()\n",
    "    \n",
    "    intersection = (pred * gt_mask).sum()\n",
    "    dice = (2. * intersection) / (pred.sum() + gt_mask.sum() + 1e-8)\n",
    "    return dice.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Val Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader, augmentations_Obj, model, optimizer, loss_fn, scheduler, scaler):\n",
    "    model.train()\n",
    "    loop = tqdm(loader, leave=False)\n",
    "    losses = []\n",
    "    dice_scores = []\n",
    "\n",
    "    for image, mask in loop:\n",
    "        # Apply Augmentations\n",
    "        image, mask = augmentations_Obj(image, mask)\n",
    "        \n",
    "        image = image.to(device=DEVICE)\n",
    "        mask = mask.float().to(device=DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(image)\n",
    "            loss = loss_fn(predictions, mask)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Calculate Dice score\n",
    "        dice_score = dice_coefficient(predictions, mask)\n",
    "        dice_scores.append(dice_score)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        loop.set_postfix(loss=sum(losses) / len(losses), diceScore=sum(dice_scores) / len(dice_scores))\n",
    "        \n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    avg_dice = sum(dice_scores) / len(dice_scores)\n",
    "    \n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    mlflow.log_metric(\"train_loss\", avg_loss)\n",
    "    mlflow.log_metric(\"train_dice_score\", avg_dice)\n",
    "    \n",
    "    print(f\"Average training loss: {avg_loss:.5f}\")\n",
    "    print(f\"Average training Dice coefficient Score: {avg_dice:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_fn(loader, model, loss_fn):\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    dice_scores = []\n",
    "    \n",
    "    loop = tqdm(loader, leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, mask in loop:\n",
    "            image = image.to(device=DEVICE)\n",
    "            mask = mask.float().to(device=DEVICE)\n",
    "            \n",
    "            predictions = model(image)\n",
    "            loss = loss_fn(predictions, mask)\n",
    "            \n",
    "            # Calculate Dice score\n",
    "            dice_score = dice_coefficient(predictions, mask)\n",
    "            dice_scores.append(dice_score)\n",
    "            \n",
    "            val_losses.append(loss.item())\n",
    "            loop.set_postfix(loss=sum(val_losses) / len(val_losses), diceScore=sum(dice_scores) / len(dice_scores))\n",
    "    \n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    avg_dice = sum(dice_scores) / len(dice_scores)\n",
    "    \n",
    "    mlflow.log_metric(\"val_loss\", avg_val_loss)\n",
    "    mlflow.log_metric(\"val_dice_score\", avg_dice)\n",
    "    \n",
    "    print(f\"Average validation loss: {avg_val_loss:.5f}\")\n",
    "    print(f\"Average validation Dice coefficient Score: {avg_dice:.5f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=f\"../Model Checkpoints/car_segmentation_{LOSS_NAME}_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define datasets\n",
    "    whole_dataset = SegmentationDataset(dirPath=dataset_path, imageDir='images/', masksDir='masks/', img_size=IMAGE_SIZE)\n",
    "    augmentations_Obj = DataAugmentation(augmentations=copy.deepcopy(AUGMENTATIONS))\n",
    "    \n",
    "    train_size = int(0.8 * len(whole_dataset))\n",
    "    val_size = len(whole_dataset) - train_size\n",
    "    train_ds, val_ds = torch.utils.data.random_split(whole_dataset, [train_size, val_size])\n",
    "    \n",
    "    print(f\"Length of Train dataset = {len(train_ds)}\")\n",
    "    print(f\"Length of Val dataset = {len(val_ds)}\")\n",
    "\n",
    "    # Define DataLoaders\n",
    "    train_dataloader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=False)\n",
    "    val_dataloader = DataLoader(val_ds, BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "    print(\"Created Train and Val dataloaders\")\n",
    "    \n",
    "    model = UNET(in_channels=3, out_channels=5).to(DEVICE)\n",
    "    loss_fn = LOSS_FUNCTION\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.1,patience=1,verbose=True)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    if LOAD_MODEL and os.path.exists(f\"../Model Checkpoints/car_segmentation_{LOSS_NAME}_checkpoint.pth.tar\"):\n",
    "        load_checkpoint(torch.load(f\"../Model Checkpoints/car_segmentation_{LOSS_NAME}_checkpoint.pth.tar\"), model)\n",
    "    \n",
    "    with mlflow.start_run() as run:\n",
    "        \n",
    "        mlflow.log_params({\"learning_rate\": LEARNING_RATE ,\"batch_size\": BATCH_SIZE, \"number_of_epochs\": NUM_EPOCHS, \"image_size\": IMAGE_SIZE, \"loss function\": LOSS_FUNCTION})\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print(f\"\\nEPOCH [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "            train_fn(train_dataloader, augmentations_Obj, model, optimizer, loss_fn, scheduler, scaler)\n",
    "            val_fn(val_dataloader, model, loss_fn)\n",
    "\n",
    "            # save model\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\":optimizer.state_dict(),\n",
    "            }\n",
    "\n",
    "            save_checkpoint(checkpoint)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
